{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--type', default='TCGA_BRCA', type=str, help='name in TCGAs')\n",
    "    parser.add_argument('--root', default='../GDC_DATA', type=str, help='path to TCGA')\n",
    "    parser.add_argument('--savepath', default='./results/BRCA_dino_scratch/vis', type=str, help='path to wsi-text pairs')\n",
    "    args, unparsed = parser.parse_known_args()\n",
    "\n",
    "    for arg in vars(args):\n",
    "        if vars(args)[arg] == 'True':\n",
    "            vars(args)[arg] = True\n",
    "        elif vars(args)[arg] == 'False':\n",
    "            vars(args)[arg] = False\n",
    "\n",
    "    return args\n",
    "\n",
    "def clean_report_brca(report):\n",
    "    report_cleaner = lambda t: (t.replace('\\n', ' ').replace('  ', ' ') \\\n",
    "        .replace('  ', ' ').replace('  ', ' ')\\\n",
    "        .replace(' 10. ', ' ').replace(' 11. ', ' ').replace(' 12. ', ' ').replace(' 13. ', ' ').replace(' 14.', ' ')    \\\n",
    "        .replace(' 1. ', ' ').replace(' 2. ', ' ') \\\n",
    "        .replace(' 3. ', ' ').replace(' 4. ', ' ').replace(' 5. ', ' ').replace(' 6. ', ' ').replace(' 7. ', ' ').replace(' 8. ', ' ') .replace(' 9. ', ' ')   \\\n",
    "        .strip().lower() + ' ').split('. ')\n",
    "    sent_cleaner = lambda t: re.sub('[#,?;*!^&_+():-\\[\\]{}]', '', t.replace('\"', '').\n",
    "                                replace('\\\\', '').replace(\"'\", '').strip().lower())\n",
    "    tokens = [sent_cleaner(sent) for sent in report_cleaner(report)]\n",
    "    report = ' . '.join(tokens) \n",
    "    return report\n",
    "print('ready')\n",
    "\n",
    "def is_idc(text):\n",
    "    if 'ductal carcinoma' in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def is_pr(text):\n",
    "    if 'positive' in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_match(reports,gt):\n",
    "    entities = []\n",
    "    entities_gt = []\n",
    "   \n",
    "    sentence = nltk.sent_tokenize(reports)\n",
    "    for sent in sentence:\n",
    "        for c in nltk.pos_tag(nltk.word_tokenize(sent)):\n",
    "           \n",
    "            if c[1].startswith('NN'):\n",
    "                if not re.sub('([^\\u0061-\\u007a])', '', c[0])=='':\n",
    "                    entities.append(c[0])\n",
    " \n",
    "    sentence = nltk.sent_tokenize(gt)\n",
    "    for sent in sentence:\n",
    "        for c in nltk.pos_tag(nltk.word_tokenize(sent)):\n",
    "            if c[1].startswith('NN'):\n",
    "                if not re.sub('([^\\u0061-\\u007a])', '', c[0])=='':\n",
    "                    entities_gt.append(c[0])\n",
    "\n",
    "    count = 0             \n",
    "    for e in entities:\n",
    "        if e in entities_gt:\n",
    "            count+=1\n",
    "    if len(entities)==0:\n",
    "        return False\n",
    "    pr = count/len(entities)\n",
    "    \n",
    "    count = 0\n",
    "    for e in entities_gt:\n",
    "        if e in entities:\n",
    "            count+=1\n",
    "    if len(entities_gt)==0:\n",
    "        return False\n",
    "    rc = count/len(entities_gt)\n",
    "    \n",
    "    f = 2*rc*pr/(rc+pr+0.00001)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score,f1_score, roc_auc_score, precision_score\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "import json\n",
    "args = get_args_parser()\n",
    "root = os.path.join(args.savepath)\n",
    "subtype_pred = []\n",
    "subtype_target = []\n",
    "pr_pred = []\n",
    "pr_target = []\n",
    "all_event_times = []\n",
    "all_estimate = []\n",
    "result={}\n",
    "fact_vol = 0\n",
    "fact_count = 0\n",
    "for file in os.listdir(root):\n",
    "    if not file.startswith('TCGA'):\n",
    "        continue\n",
    "    file_name = os.path.join(root,file)\n",
    "    #print(file_name)\n",
    "    with open(file_name) as f:\n",
    "        data = json.loads(f.read())\n",
    "        for item in data:\n",
    "            #brca subtyping\n",
    "            tgt = item['gts']\n",
    "            predict = item['res']\n",
    "            #fact entity reward\n",
    "            if True:\n",
    "                fact = entity_match(predict,tgt)\n",
    "                if fact:\n",
    "                    fact_vol+=fact\n",
    "                    fact_count+=1 \n",
    "                    \n",
    "            if 'logical' in item['Question'][0]:\n",
    "                tgt = item['gts']\n",
    "                if not ('ductal carcinoma' in tgt or 'lobular carcinoma' in tgt):\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                subtype_pred.append(is_idc(predict))\n",
    "                subtype_target.append(is_idc(tgt))\n",
    "            #pr prediction\n",
    "            if 'receptor' in item['Question'][0]:\n",
    "                tgt = item['gts']\n",
    "                if not tgt in ('negative','positive'):\n",
    "\n",
    "                    continue\n",
    "                \n",
    "                pr_pred.append(is_pr(predict))\n",
    "                pr_target.append(is_pr(tgt))\n",
    "                \n",
    "            if 'survival time' in item['Question'][0]:\n",
    "                res = item['res']\n",
    "                gts = item['gts']\n",
    "\n",
    "                if not res.isdecimal():\n",
    "                    continue\n",
    "                all_event_times.append(eval(gts))\n",
    "                all_estimate.append(eval(res))\n",
    "                \n",
    "\n",
    "\n",
    "r = recall_score(subtype_pred, subtype_target)\n",
    "f1 =f1_score(subtype_pred, subtype_target)\n",
    "p = precision_score(subtype_pred, subtype_target)\n",
    "\n",
    "pr_r = recall_score(pr_pred, pr_target)\n",
    "pr_p = precision_score(pr_pred, pr_target)\n",
    "pr_f1 = f1_score(pr_pred, pr_target)\n",
    "print(subtype_pred)\n",
    "print(subtype_target)\n",
    "cindex = 1-concordance_index_censored([True]*len(all_estimate), all_event_times, all_estimate, tied_tol=1e-08)[0]\n",
    "result.update({'subtype_r':r, 'subtype_p':p,'subtype_f1':f1, 'pr_r':pr_r,'pr_p':pr_p, 'pr_f1':pr_f1,'fact':fact_vol/fact_count})\n",
    "print(result)\n",
    "print(f'cindex:{cindex}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
